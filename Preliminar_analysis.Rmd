---
title: "Preliminar analysis and recoding"
author: "Riccardo Ruta"
date: "7/5/2022"
output:
  pdf_document: 
    toc: yes
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source(here::here("src","00_setup.R"))
```

## 1) First import the dataset and check variables

```{r}
# import the data
tw <-  read_csv("data/large_files/politicians_all_final_tweets.csv", show_col_types = FALSE )

kable(colnames(tw), col.names = "variables")
```

## 2) Adjust date.time format

```{r, results = "hide"}
Sys.setlocale("LC_TIME", "C")
tw$date <- as.Date(strptime(tw$creato_il,"%a %b %d %H:%M:%S %z %Y", tz = "CET"))
tw$date <- na.replace(tw$date, as.Date(tw$creato_il))
```

### Check the conversion

```{r}
check_dates <- tw %>% select(creato_il,date)
kable(head(check_dates), col.names = c("Old date", "New date"))
kable(tail(check_dates), col.names = c("Old date", "New date"))
```

## 3) Create the week variable

```{r}
tw <- tw %>% mutate(week = cut.Date(date, breaks = "1 week", labels = FALSE))
```

#### Inspect the first and the last dates and check if the number of weeks is correct

```{r}
max(tw$date)
min(tw$date)
difftime(max(tw$date), min(tw$date), units = "weeks")
```

### Create the month variable

```{r}
tw <- tw %>% mutate(month = cut.Date(date, breaks = "1 month", labels = FALSE))
```

#### Check the number of month

```{r}
max(tw$month)
length(seq(from = min(tw$date), to = max(tw$date), by = 'month'))
```

### Count the number of missing values

```{r}
sum(is.na(tw))
```

### Inspect where are those missings

```{r}

missings <- c(
sum(is.na(tw$tw_screen_name)),
sum(is.na(tw$nome)),
sum(is.na(tw$tweet_testo)),
sum(is.na(tw$creato_il)),
sum(is.na(tw$creato_il_code)),
sum(is.na(tw$url)),
sum(is.na(tw$party_id)),
sum(is.na(tw$genere)),
sum(is.na(tw$chamber)),
sum(is.na(tw$status)),
sum(is.na(tw$date)),
sum(is.na(tw$week)),
sum(is.na(tw$month)) )

missing_df <- data.frame(colnames(tw), missings)
kable(missing_df)

```

### From that analysis i obtain 147306 url missing, this is because the url is collected only when the tweets has an external link to other sources, for our analysis we can ignore those missings, with this check also results 6494 tweets missing those are the cases when someone post only images or video without text, so the extraction is correct.

## 4) Remove the rows with missing tweets

```{r}
sum(is.na(tw$tweet_testo))
tw <- tw %>% drop_na(tweet_testo)
```

## 5) Inspect that the variables correspond to the expectation

```{r}
unique(tw$party_id)
unique(tw$genere)
unique(tw$chamber)
unique(tw$status)
```

### The variable genere needs to be corrected

```{r}
# Remove space from genere variable [RUN ONLY ONCE!]
a <- unique(tw$genere)
a[3]
which(tw$genere == a[3])
tw$genere <- gsub(a[3],"male",tw$genere)
```

### Check the substitution

```{r}
which(tw$genere == a[3])
unique(tw$genere)
```

### Now all the variables are ready for next steps

## 6) Create a new dataset with only necessary informations

```{r}
# Select variables for the analysis
dataset <- tw %>% select(nome, tweet_testo, genere, party_id,chamber,status, date, week, month )
colnames(dataset)
```

## 7) Create the corpus

```{r}
corpus <- corpus(dataset, text = "tweet_testo")
ndoc(corpus)
```

## 8) Create the DFM

```{r}
# Split the corpus into single tokens (remain positional)
doc.tokens <- tokens(corpus,
                                 remove_punct = TRUE,
                                 remove_numbers = TRUE,
                                 remove_symbols = TRUE,
                                 remove_url = TRUE)

# Import my stopwords
my_word <- as.list(read_csv("data/it_stopwords_new_list.csv",
                            show_col_types = FALSE))

# Attach unrecognized symbols
my_list <- c("ðŸ‡®ðŸ‡¹","c'Ã¨","+","ðŸ”´", my_word$stopwords, stopwords('italian'))

doc.tokens <- tokens_select(doc.tokens, my_list, selection='remove')

DFM <- dfm(doc.tokens)

# Check the topfeatures
topfeatures(DFM,15)
```

## 9) Trim the data

### Only words that occur in the top 20% of the distribution and in less than 30% of documents. Very frequent but document specific words.

```{r}
DFM_trimmed <- dfm_trim(DFM, min_termfreq = 0.80, termfreq_type = "quantile",
                        max_docfreq = 0.3, docfreq_type = "prop")
```

### Now the data are ready for the next analysis

```{r, include=FALSE, eval=FALSE}
save(DFM_trimmed,file="data/dfm_trimmed.Rda")
```

## 10) Some preliminar analysis

### Topfeatures frquency

```{r}
# Plot frequency of the topfeatures in the original DFM
features_dfm <- textstat_frequency(DFM, n = 20)
features_dfm

ggplot(features_dfm, aes(x = feature, y = frequency)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Plot frequency of the topfeatures in the trimmed DFM
features_dfm_trimmed <- textstat_frequency(DFM_trimmed, n=20)
features_dfm_trimmed

ggplot(features_dfm_trimmed, aes(x = feature, y = frequency)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

### Most common hashtag

```{r}
tag_dfm <- dfm_select(DFM, pattern = "#*")
toptag <- names(topfeatures(tag_dfm, 20))
head(toptag)
```

### Co-occurrence matrix of hashtag

```{r}
tag_fcm <- fcm(tag_dfm)

topgat_fcm <- fcm_select(tag_fcm, pattern = toptag)
textplot_network(topgat_fcm, min_freq = 0.1, edge_alpha = 0.8, edge_size = 5)
```

#### Extract the most popular hashtags from the original dataset

#### We'll use regular expressions to extract hashtags.

```{r}
ht <- str_extract_all(dataset$tweet_testo, '#[A-Za-z0-9_]+')
ht <- unlist(ht)
head(sort(table(ht), decreasing = TRUE))
```

### Extract most frequently mentioned usernames

```{r}
user_dfm <- dfm_select(DFM, pattern = "@*")
topuser <- names(topfeatures(user_dfm, 20))
head(topuser)
```

### Feature-occurrence matrix of usernames

```{r}
user_fcm <- fcm(user_dfm)

user_fcm <- fcm_select(user_fcm, pattern = topuser)
textplot_network(user_fcm, min_freq = 0.1, edge_color = "orange", edge_alpha = 0.8, edge_size = 5)
```
