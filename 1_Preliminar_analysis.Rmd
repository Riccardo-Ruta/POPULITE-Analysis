---
title: "Data cleaning and Preliminar analysis"
author: "Riccardo Ruta"
date: "5/2022"
output:
  pdf_document: 
    toc: yes
    latex_engine: xelatex
---

```{r setup_1, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source(here::here("src","00_setup.R"))
```

# Data cleaning

## Import the dataset and check variables

```{r}
# import the data
tw <-  read_csv("data/large_files/politicians_final_corrected.csv",
                show_col_types = FALSE )

kable(colnames(tw), col.names = "variables")
```

## Adjust date.time format

```{r, results = "hide"}
# RUN IN THIS ORDER !!
Sys.setlocale("LC_TIME", "C")
tw$date <- as.Date(strptime(tw$creato_il,"%a %b %d %H:%M:%S %z %Y",
                            tz = "CET"))
tw$date <- na.replace(tw$date, as.Date(tw$creato_il))
```

### Check the conversion

```{r}
check_dates <- tw %>% select(creato_il,date)
kable(head(check_dates), col.names = c("Old date", "New date"))
kable(tail(check_dates), col.names = c("Old date", "New date"))
```

## Create the week variable

```{r}
tw <- tw %>% mutate(week = cut.Date(date, breaks = "1 week", labels = FALSE))
```

### Check the variable 

Inspect the first and the last dates and check if the number of weeks is correct

```{r}
max(tw$date)
min(tw$date)
difftime(max(tw$date), min(tw$date), units = "weeks")
```

## Create the month variable

```{r}
tw <- tw %>% mutate(month = cut.Date(date, breaks = "1 month", labels = FALSE))
```

### Check the number of month

```{r}
max(tw$month)
length(seq(from = min(tw$date), to = max(tw$date), by = 'month'))
```

## Count the number of missing values

```{r}
sum(is.na(tw))
```

### Inspect where are the missings

```{r}

missings <- c(
sum(is.na(tw$tw_screen_name)),
sum(is.na(tw$nome)),
sum(is.na(tw$tweet_testo)),
sum(is.na(tw$creato_il)),
sum(is.na(tw$creato_il_code)),
sum(is.na(tw$url)),
sum(is.na(tw$party_id)),
sum(is.na(tw$genere)),
sum(is.na(tw$chamber)),
sum(is.na(tw$status)),
sum(is.na(tw$date)),
sum(is.na(tw$week)),
sum(is.na(tw$month)) )

missing_df <- data.frame(colnames(tw), missings)
kable(missing_df)

```

From that analysis i obtain 148178 url missing, this is because the url is collected only when the tweets has an external link to other sources, for our analysis we can ignore those missings, with this check also results 6494 tweets missing those are the cases when someone post only images or video without text, so the extraction is correct.

### Remove rows with missing tweets

```{r}
sum(is.na(tw$tweet_testo))
tw <- tw %>% drop_na(tweet_testo)
```

## Check that the variables make sense

```{r}
unique(tw$party_id)
unique(tw$genere)
unique(tw$chamber)
unique(tw$status)
```

### Adjust the variable genere

```{r}
# Remove space from genere variable [RUN ONLY ONCE!]
a <- unique(tw$genere)
a[3]
which(tw$genere == a[3])
tw$genere <- gsub(a[3],"male",tw$genere)
```

### Verify the substitution

```{r}
which(tw$genere == a[3])
unique(tw$genere)
```



**Now all the variables are ready for next steps**

## Create a new dataset selecting only necessary informations

```{r}
# Select variables for the analysis
dataset <- tw %>% select(nome, tweet_testo, genere, party_id,
                         chamber,status, date, week, month )
colnames(dataset)
```

```{r, include=FALSE, eval=FALSE}
#save(dataset,file="data/dataset.Rda")
```

## Create the corpus

```{r}
corpus <- corpus(dataset, text = "tweet_testo")
ndoc(corpus)
```

```{r, include=FALSE, eval=FALSE}
#save(corpus,file="data/corpus.Rda")
```

## Create the DFM

```{r}
# Split the corpus into single tokens (remain positional)
doc.tokens <- tokens(corpus,
                                 remove_punct = TRUE,
                                 remove_numbers = TRUE,
                                 remove_symbols = TRUE,
                                 remove_url = TRUE)

# Import my stopwords
my_word <- as.list(read_csv("data/it_stopwords_new_list.csv",
                            show_col_types = FALSE))

# Attach unrecognized symbols
my_list <- c("ðŸ‡®ðŸ‡¹","c'Ã¨","+","ðŸ”´", my_word$stopwords,
             stopwords('italian'), stopwords("english"))

# Save my_list
#save(my_list,file="data/my_list.Rda")

doc.tokens <- tokens_select(doc.tokens, my_list, selection='remove')

DFM <- dfm(doc.tokens, tolower = TRUE)
```

## Trim the data

Only words that occur in the top 20% of the distribution and in less than 30% of documents. Very frequent but document specific words.

```{r}
DFM_trimmed <- dfm_trim(DFM, min_termfreq = 0.80, termfreq_type = "quantile",
                        max_docfreq = 0.3, docfreq_type = "prop")

# Check the topfeatures
topfeatures(DFM_trimmed,15)
```

## Remove the emoji

```{r, results="hide"}
# Create a copy of the dfm
test <- DFM_trimmed
# Remove from the copy all the non ASCII carachters
test@Dimnames$features <- gsub("[^\x01-\x7F]", "", test@Dimnames$features)

# Check the difference from the list of features before and after apply gsub
a <- unique(test@Dimnames$features)
b <- unique(DFM_trimmed@Dimnames$features)
setdiff(b,a) #I have selected also words that cannot be removed

# Create an object with the features after remove non ASCII characters
c <- test@Dimnames$features
# Create an object with the original features
d <- DFM_trimmed@Dimnames$features

# Create the list of the removed features
diff <- setdiff(d,c)
emoji <- diff[diff %>% nchar() < 4] 
emoji <- list(emoji)
# Now i can remove this list from the dfm
DFM_trimmed <- dfm_remove(DFM_trimmed, emoji)

#save(DFM_trimmed,file="data/dfm_trimmed.Rda")
```

![Emoji removed](output/emoji.png)

## Take the proportion of the frequencies

```{r}
# Weight the frequency
dfm_weight <- DFM_trimmed %>%
  dfm_weight(scheme = "prop")
```

### Now the data are ready for the next analysis

```{r, include=FALSE, eval=FALSE}
#save(dfm_weight,file="data/dfm_weight.Rda")
```

\newpage

# Preliminar analysis

## Who is inside this dataset?

```{r}
# Number of parliamentarians
n_parl <- length(unique(dataset$nome))
n_parl
# How many parliamentarians for each party?
n_parl_party <- dataset %>% group_by(party_id) %>% count()
kable(n_parl_party)

# Gneder composition
n_gender <- dataset %>% group_by(genere) %>% count()
kable(n_gender)

# Wich is the period of analysis?
max(tw$date)
min(tw$date)
```

## Topfeatures frquency

```{r, warning=FALSE, echo=FALSE}
# Textplotwordcloud
set.seed(123)
textplot_wordcloud(dfm_weight,min_count = 20,max_words = 300,
     color = c('red', 'pink', 'green', 'purple', 'blue'))
```

```{r, echo=FALSE}
# Plot frequency of the topfeatures in the DFM
features_dfm <- textstat_frequency(dfm_weight, n = 50)

# Sort by reverse frequency order
features_dfm$feature <- with(features_dfm, reorder(feature, -frequency))

ggplot(features_dfm, aes(x = feature, y = frequency)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))


```

### Relative frequency of the topfeatures by Party ID

```{r,echo=FALSE}
#kable(unique(DFM_trimmed$party_id),col.names = "Party")
```

```{r, echo=FALSE}
# Plot relative frequency by party_id
freq_weight <- textstat_frequency(dfm_weight, n = 7, 
                                  groups = dfm_weight$party_id)

ggplot(data = freq_weight, aes(x = nrow(freq_weight):1, y = frequency)) +
     geom_point() +
     facet_wrap(~ group, scales = "free") +
     coord_flip() +
     scale_x_continuous(breaks = nrow(freq_weight):1,
                        labels = freq_weight$feature) +
     labs(x = NULL, y = "Relative frequency")
```

## Most common hashtag

```{r}
tag_dfm <- dfm_select(dfm_weight, pattern = "#*")
toptag <- names(topfeatures(tag_dfm, 20))
toptag
```

```{r, echo=FALSE}
tag_dfm %>% 
  textstat_frequency(n = 20) %>% 
  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
  geom_point() +
  coord_flip() +
  labs(x = NULL, y = "Frequency") +
  theme_minimal()
```

### Most common hashtag by Gender

```{r}
tstat_freq <- textstat_frequency(tag_dfm, n = 20,
                                 groups = dfm_weight$genere)
```

```{r, echo=FALSE}
# Plot relative frequency by Gender
ggplot(data = tstat_freq, aes(x = nrow(tstat_freq):1, y = frequency)) +
     geom_point() +
     facet_wrap(~ group, scales = "free") +
     coord_flip() +
     scale_x_continuous(breaks = nrow(tstat_freq):1,
                        labels = tstat_freq$feature) +
     labs(x = NULL, y = "Relative frequency")
```

### Co-occurrence Plot of hashtags

Not weighted
```{r, echo=FALSE}
# NOT WEIGHTED
tag_dfm_NOT_W <- dfm_select(DFM, pattern = "#*")
toptag_NOT <- names(topfeatures(tag_dfm_NOT_W, 20))

tag_fcm_NOT <- fcm(tag_dfm_NOT_W)
set.seed(666)
topgat_fcm_NOT <- fcm_select(tag_fcm_NOT, pattern = toptag_NOT)
textplot_network(topgat_fcm_NOT, min_freq = 0.1, edge_alpha = 0.8, edge_size = 5)
```

Weighted
```{r, echo=FALSE}
# WEIGHTED
tag_fcm <- fcm(tag_dfm)
set.seed(123)
topgat_fcm <- fcm_select(tag_fcm, pattern = toptag)
textplot_network(topgat_fcm)#, min_freq = 0.1, edge_alpha = 0.8, edge_size = 5)
```

```{r, eval=FALSE, include=FALSE}
# Check most frequent hashtag extracted with regular expressions
ht <- str_extract_all(dataset$tweet_testo, '#[A-Za-z0-9_]+')
ht <- unlist(ht)
head(sort(table(ht), decreasing = TRUE))
```

## Most frequently mentioned usernames

```{r}
user_dfm <- dfm_select(dfm_weight, pattern = "@*")
topuser <- names(topfeatures(user_dfm, 20, scheme = "docfreq"))
kable(topuser, col.names = "Most mentioned username")
```

### Most frequently mentioned usernames by gender

```{r}
user_tstat_freq <- textstat_frequency(user_dfm, n = 20,
                                 groups = dfm_weight$genere)
```

```{r, echo=FALSE}
# Plot relative frequency by Gender
ggplot(data = user_tstat_freq, aes(x = nrow(user_tstat_freq):1, y = frequency)) +
     geom_point() +
     facet_wrap(~ group, scales = "free") +
     coord_flip() +
     scale_x_continuous(breaks = nrow(user_tstat_freq):1,
                        labels = user_tstat_freq$feature) +
     labs(x = NULL, y = "Relative frequency")
```

### Co-occurrence plot of usernames

Weighted
```{r, echo=FALSE}
# WEIGHTED
user_fcm <- fcm(user_dfm)
set.seed(123)
user_fcm <- fcm_select(user_fcm, pattern = topuser)
textplot_network(user_fcm, min_freq = 0.1, edge_color = "orange", edge_alpha = 0.8, edge_size = 5)
```

Not weighted
```{r}
# NOT WEIGHTED
user_dfm_NOT_W <- dfm_select(DFM, pattern = "@*")
topuser_NOT <- names(topfeatures(user_dfm_NOT_W, 20, scheme = "docfreq"))

user_fcm_NOT <- fcm(user_dfm_NOT_W)
set.seed(6)
topuser_fcm_NOT <- fcm_select(user_fcm_NOT, pattern = topuser_NOT)
textplot_network(topuser_fcm_NOT, min_freq = 0.1, edge_alpha = 0.8, edge_size = 5)
```

## How many times a politician cite his/her party
```{r, include=FALSE}
load("data/party_citations.Rda")
```


```{r, eval =FALSE}
party_citations <- data.frame(first = vector(), second = vector() )
system.time(
for (i in unique(tw$party_id))
{
  a <- paste("#", i ,sep = "")
  b <- tw %>% filter(grepl(a,tweet_testo)&party_id== i) %>% count()
  c <- tw %>% filter(party_id == i) %>% count()
  d <- (b/c) * 100
  party_citations <- rbind(party_citations, cbind(i,b,c,d))

}
)

#save(party_citations, file = "data/party_citations.Rda")
```

```{r}
colnames(party_citations) <- c("party","n_citations", "Number of tweets", "perc")

kable(party_citations %>% arrange(desc(perc)), col.names = c("Party","Number of citations",
                                     "number of tweets", "% of citations"))
```
In the above script i search the # for the parliamentary group, but is very unlikely, for example, that someone use the #IV for talking about the "Italia Viva" party, so i decided to enrich the dataframe creating a new variable with the name of the official twitter page for every party, and repeat the search using it.

I created the variable party_Page for only those parliamentary group that has a direct connection with a party (i excluded Reg_leagues, misto and indipendente)

### Create the variable with the name of the official Twitter account

```{r}
tw <- tw %>% mutate(party_page = if_else(party_id == "PD", "@pdnetwork",
if_else( party_id == "FDI", "@FratellidItalia",
      if_else(party_id == "M5S", "@Mov5Stelle",
              if_else(party_id == "FI", "@forza_italia",
                      if_else(party_id == "LEGA", "@LegaSalvini",
                              if_else(party_id == "IV", "@ItaliaViva",
                                      if_else(party_id == "CI", "@coraggio_italia",
                                              if_else(party_id == "LEU", "@liberi_uguali",
                                                      "NA")))))))))

```

### Count for each party how many times a politician cite their respective party
```{r, include=FALSE}
load("data/party_citations_page.Rda")
```

```{r,eval=FALSE}
party_citations_page <- data.frame(first = vector(), second = vector(),
                                  third = vector(), fourth = vector())
system.time(
  for (i in unique(tw$party_page))
  {
    b <- tw %>% filter(grepl(i,tweet_testo)&party_page== i) %>% count()
    c <- tw %>% filter(party_page == i) %>% count()
    d <- (b/c) * 100
    party_citations_page <- rbind(party_citations_page, cbind(i,b,c,d))
    
  }
)
# save(party_citations_page, file = "data/party_citations_page.Rda")
```

```{r}
colnames(party_citations_page) <- c("party","n_citations", "Number of tweets", "perc")

kable(party_citations_page %>% filter(party != "NA") %>%arrange(desc(perc)), col.names = c("Party","Number of citations",
                                          "number of tweets", "% of citations"))
```



## How many times a politician cite itself in the tweet
```{r, include=FALSE}
load("data/self_citations.Rda")
```

```{r, eval=FALSE}
self_citations <- data.frame(first = vector(), second = vector() )
system.time(
for (i in unique(tw$tw_screen_name))
{
  a <- paste("@", i ,sep = "")
  b <- tw %>% filter(grepl(a,tweet_testo) & tw_screen_name== i) %>% count()
  c <- tw %>% filter(tw_screen_name == i) %>% count()
  d <- (b/c) * 100
  self_citations <- rbind(self_citations, cbind(i,b,c,d))
}
)
#save(self_citations, file = "data/self_citations.Rda")
```

```{r}
colnames(self_citations) <- c("Politician","n_citations", "Number of tweets", "perc")

kable(self_citations %>% filter(n_citations > 2) %>% arrange(desc(perc)), col.names = c("Politician","Number of citations",
                                                     "Number of tweets", "% of citations"))
```

