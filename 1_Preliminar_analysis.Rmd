---
title: "Preliminar analysis and recoding"
author: "Riccardo Ruta"
date: "5/2022"
output:
  pdf_document: 
    toc: yes
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source(here::here("src","00_setup.R"))
```

## 1) First import the dataset and check variables

```{r}
# import the data
tw <-  read_csv("data/large_files/politicians_all_final_tweets.csv", show_col_types = FALSE )

kable(colnames(tw), col.names = "variables")
```

## 2) Adjust date.time format

```{r, results = "hide"}
Sys.setlocale("LC_TIME", "C")
tw$date <- as.Date(strptime(tw$creato_il,"%a %b %d %H:%M:%S %z %Y", tz = "CET"))
tw$date <- na.replace(tw$date, as.Date(tw$creato_il))
```

### Check the conversion

```{r}
check_dates <- tw %>% select(creato_il,date)
kable(head(check_dates), col.names = c("Old date", "New date"))
kable(tail(check_dates), col.names = c("Old date", "New date"))
```

## 3) Create the week variable

```{r}
tw <- tw %>% mutate(week = cut.Date(date, breaks = "1 week", labels = FALSE))
```

#### Inspect the first and the last dates and check if the number of weeks is correct

```{r}
max(tw$date)
min(tw$date)
difftime(max(tw$date), min(tw$date), units = "weeks")
```

### Create the month variable

```{r}
tw <- tw %>% mutate(month = cut.Date(date, breaks = "1 month", labels = FALSE))
```

#### Check the number of month

```{r}
max(tw$month)
length(seq(from = min(tw$date), to = max(tw$date), by = 'month'))
```

### Count the number of missing values

```{r}
sum(is.na(tw))
```

### Inspect where are those missings

```{r}

missings <- c(
sum(is.na(tw$tw_screen_name)),
sum(is.na(tw$nome)),
sum(is.na(tw$tweet_testo)),
sum(is.na(tw$creato_il)),
sum(is.na(tw$creato_il_code)),
sum(is.na(tw$url)),
sum(is.na(tw$party_id)),
sum(is.na(tw$genere)),
sum(is.na(tw$chamber)),
sum(is.na(tw$status)),
sum(is.na(tw$date)),
sum(is.na(tw$week)),
sum(is.na(tw$month)) )

missing_df <- data.frame(colnames(tw), missings)
kable(missing_df)

```

### From that analysis i obtain 147306 url missing, this is because the url is collected only when the tweets has an external link to other sources, for our analysis we can ignore those missings, with this check also results 6494 tweets missing those are the cases when someone post only images or video without text, so the extraction is correct.

## 4) Remove the rows with missing tweets

```{r}
sum(is.na(tw$tweet_testo))
tw <- tw %>% drop_na(tweet_testo)
```

## 5) Inspect that the variables correspond to the expectation

```{r}
unique(tw$party_id)
unique(tw$genere)
unique(tw$chamber)
unique(tw$status)
```

### The variable genere needs to be corrected

```{r}
# Remove space from genere variable [RUN ONLY ONCE!]
a <- unique(tw$genere)
a[3]
which(tw$genere == a[3])
tw$genere <- gsub(a[3],"male",tw$genere)
```

### Check the substitution

```{r}
which(tw$genere == a[3])
unique(tw$genere)
```

### Now all the variables are ready for next steps

## 6) Create a new dataset with only necessary informations

```{r}
# Select variables for the analysis
dataset <- tw %>% select(nome, tweet_testo, genere, party_id,chamber,status, date, week, month )
colnames(dataset)
```

```{r, include=FALSE, eval=FALSE}
#save(dataset,file="data/dataset.Rda")
```

## 7) Create the corpus

```{r}
corpus <- corpus(dataset, text = "tweet_testo")
ndoc(corpus)
```

```{r, include=FALSE, eval=FALSE}
#save(corpus,file="data/corpus.Rda")
```

## 8) Create the DFM

```{r}
# Split the corpus into single tokens (remain positional)
doc.tokens <- tokens(corpus,
                                 remove_punct = TRUE,
                                 remove_numbers = TRUE,
                                 remove_symbols = TRUE,
                                 remove_url = TRUE)

# Import my stopwords
my_word <- as.list(read_csv("data/it_stopwords_new_list.csv",
                            show_col_types = FALSE))

# Attach unrecognized symbols
my_list <- c("ðŸ‡®ðŸ‡¹","c'Ã¨","+","ðŸ”´", my_word$stopwords, stopwords('italian'))

# Save my_list
#save(my_list,file="data/my_list.Rda")

doc.tokens <- tokens_select(doc.tokens, my_list, selection='remove')

DFM <- dfm(doc.tokens, tolower = TRUE)

# Check the topfeatures
topfeatures(DFM,15)
```

## 9) Trim the data

### Only words that occur in the top 20% of the distribution and in less than 30% of documents. Very frequent but document specific words.

```{r}
DFM_trimmed <- dfm_trim(DFM, min_termfreq = 0.80, termfreq_type = "quantile",
                        max_docfreq = 0.3, docfreq_type = "prop")
```

### Now the data are ready for the next analysis

```{r, include=FALSE, eval=FALSE}
#save(DFM_trimmed,file="data/dfm_trimmed.Rda")
```

## 10) Some preliminar analysis

### Topfeatures frquency

```{r}
# Plot frequency of the topfeatures in the DFM
features_dfm <- textstat_frequency(DFM, n = 50)
head(features_dfm)

# Sort by reverse frequency order
features_dfm$feature <- with(features_dfm, reorder(feature, -frequency))

ggplot(features_dfm, aes(x = feature, y = frequency)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

### Relative frequency of the topfeatures by Party ID

```{r}
kable(unique(DFM_trimmed$party_id),col.names = "Party")
```

```{r}
# Weight the frequency
dfm_weight_pres <- DFM_trimmed %>%
  dfm_weight(scheme = "prop")

# Calculate relative frequency by president
freq_weight <- textstat_frequency(dfm_weight_pres, n = 5, 
                                  groups = dfm_weight_pres$party_id)

ggplot(data = freq_weight, aes(x = nrow(freq_weight):1, y = frequency)) +
     geom_point() +
     facet_wrap(~ group, scales = "free") +
     coord_flip() +
     scale_x_continuous(breaks = nrow(freq_weight):1,
                        labels = freq_weight$feature) +
     labs(x = NULL, y = "Relative frequency")
```

### Most frequent words by gender

```{r}
dfm_gender <- dfm_group(DFM_trimmed, groups = genere)

textplot_wordcloud(dfm_gender[c("male", "female"), ], 
                   max_words = 80, comparison = TRUE)
```

### Most frequent words by Chamber

```{r, warning=FALSE}
dfm_chamber <- dfm_group(DFM_trimmed, groups = chamber)

textplot_wordcloud(dfm_chamber[c("Camera", "Senate", "NotParl"), ],
                   max_words = 80, comparison = TRUE)
```

### Most common hashtag

```{r}
tag_dfm <- dfm_select(DFM, pattern = "#*")
toptag <- names(topfeatures(tag_dfm, 20))
head(toptag)
```

```{r}
tstat_freq <- textstat_frequency(tag_dfm, n = 5, groups = genere)
head(tstat_freq, 20)
```

```{r}
tag_dfm %>% 
  textstat_frequency(n = 20) %>% 
  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
  geom_point() +
  coord_flip() +
  labs(x = NULL, y = "Frequency") +
  theme_minimal()
```

### Co-occurrence Plot of hashtag

```{r}
tag_fcm <- fcm(tag_dfm)

topgat_fcm <- fcm_select(tag_fcm, pattern = toptag)
textplot_network(topgat_fcm, min_freq = 0.1, edge_alpha = 0.8, edge_size = 5)
```

#### Check most frequent hashtag extracted with regular expressions

```{r}
ht <- str_extract_all(dataset$tweet_testo, '#[A-Za-z0-9_]+')
ht <- unlist(ht)
head(sort(table(ht), decreasing = TRUE))
```

### Extract most frequently mentioned usernames

```{r}
user_dfm <- dfm_select(DFM, pattern = "@*")
topuser <- names(topfeatures(user_dfm, 20))
head(topuser)
```

### Feature-occurrence plot of usernames

```{r}
user_fcm <- fcm(user_dfm)

user_fcm <- fcm_select(user_fcm, pattern = topuser)
textplot_network(user_fcm, min_freq = 0.1, edge_color = "orange", edge_alpha = 0.8, edge_size = 5)
```
